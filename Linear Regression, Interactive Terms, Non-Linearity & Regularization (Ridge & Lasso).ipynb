{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41163d3a",
   "metadata": {},
   "source": [
    "# Linear Regression in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b6881",
   "metadata": {},
   "source": [
    "### Fitting a line for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ad5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba43a4",
   "metadata": {},
   "source": [
    "### Loading the boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "438b7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=load_boston()\n",
    "features=boston.data[:,0:2]\n",
    "target=boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a20ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\rutuj\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6704bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Linear Regression\n",
    "lin_reg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a010157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the linear regression\n",
    "model=lin_reg.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa7a4e",
   "metadata": {},
   "source": [
    "In Boston dataset, only 2 predictors are considered. Therefore, y=B0+B1X1+b2X2+e where B0 intercept, e is ther error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049108da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35207832,  0.11610909])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the feature coefficients\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d94f061f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.485628113468223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the intercept\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bba519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In dataset, target value is the median value of Boston home in thousands of dollars. Therefore, price of first home in the dataset is multiplied by 1000\n",
    "target[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613e4712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24573.366631705547"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the target value of the first observation, multiplied by 1000\n",
    "model.predict(features)[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a06c71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-352.0783156402677"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First co-efficient multiplied by 1000\n",
    "model.coef_[0]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bbbe1",
   "metadata": {},
   "source": [
    "1st feature represents number of crimes per capita. -352.07 Indicates that every single crime per capita will decrease the price of the house by approximately $350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022c95a",
   "metadata": {},
   "source": [
    "### Handling Interactive Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25e0d7",
   "metadata": {},
   "source": [
    "In reality, we have features whose effect on the target variables depends on another feature.To capture this interaction effect in our model, we can use scikit-learn's PolynomialFeature() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18831d5",
   "metadata": {},
   "source": [
    "Example: We have to predict if the coffee tastes sweet, and we have 2 binary features: sugar (if sugar is present or not) and stirred (if we have stirred or not).The coffee will taste sweet only when both sugar=1 and stirred =1. Just putting sugar in the coffee(sugar=1,stirred=0) OR just stirring the coffee(sugar=0,stirred=1) won't make the coffee taste sweet. Thus, the effect of sugar and coffee are dependent on each other. In such a case, we say there is an interaction effect between features sugar and stirred. This interaction effect can be included in our linear regression model, by including a new feature comprising product of corresponding values from the interacting features: y=B0+ B1X1 + B2X2 + B3X1X2 + e. Now, for practical understanding, let's implement this on our boston dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e762d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "665df279",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=load_boston()\n",
    "features=boston.data[:,0:2]\n",
    "target=boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "650b696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create interaction term\n",
    "interaction=PolynomialFeatures(degree=3,include_bias=False,interaction_only=True)\n",
    "features_interaction=interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af2597",
   "metadata": {},
   "source": [
    "By default, polynomial features will add a bias, which can be dropped by include_bias=False. interaction_only=True tells the Polynomial to include only interaction terms (x1.x2) and not the PolynomialFeatures(X1^2, x2^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6f79558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.3200e-03, 1.8000e+01, 1.1376e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_interaction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12b5b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the regression with interactive features\n",
    "regression=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d6d2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=regression.fit(features_interaction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51e136",
   "metadata": {},
   "source": [
    "### Fitting a Non-Linear Relatipnship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423f105",
   "metadata": {},
   "source": [
    "In reality, many a times, our data is non-linear.This non-linearity in data can also be captured using PolynomialFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e850fa",
   "metadata": {},
   "source": [
    "Example of Linear data: number of stories a building has and the buidling's height. In Linear regression, we assume that the effect of number of storeis and buidling height is constant. A 20 story building height will be roughly twice as high as a 10-story building, which will be twice as high as a 5-story building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565076ec",
   "metadata": {},
   "source": [
    "Example of Non-linear data: Relationship between the number of hours a student studies and the marks he/she gets in a test. Intuitively, one can imagine there is a big difference in the test scores between students who study for 1 hour as compared to students who did not study at all. However, there is much smaller difference in test scores between a student who studied for 99 hours and a student who studied for 100 hours. The effect of one hour of studying that has on student's test scores, decreases as the number of hours increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d90d9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linearity=PolynomialFeatures(degree=3,include_bias=False)\n",
    "non_linear_features=non_linearity.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88888856",
   "metadata": {},
   "source": [
    "Here, by default, interaction_term=False and hence all terms are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95222790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.32000000e-03, 1.80000000e+01, 3.99424000e-05, 1.13760000e-01,\n",
       "       3.24000000e+02, 2.52435968e-07, 7.18963200e-04, 2.04768000e+00,\n",
       "       5.83200000e+03])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b339286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_reg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5e18839",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_model=non_linear_reg.fit(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e892b5",
   "metadata": {},
   "source": [
    "### Using Regularization to Reduce Overfitting (reduce variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412ed67",
   "metadata": {},
   "source": [
    "Regularized regression learners attempt to minimize RSS=(summation of (y-y')^2) and shrinkage penalty (some penalty for total size of coefficient values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc026b9",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a93d0",
   "metadata": {},
   "source": [
    "In ridge regression, the shrinkage penalty is a hyperparameter multiplied by the squared sum of all coefficients: RSS + alpha * summation of B^2 (for all co-efficients B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52ac8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd5d948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "boston=load_boston()\n",
    "features=boston.data\n",
    "target=boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a23b4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "scaler=StandardScaler()\n",
    "features_standardized=scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d3c265dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ridge refression with alpha value\n",
    "regression_r=Ridge(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65d03748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the linear regression\n",
    "model=regression_r.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a205f2",
   "metadata": {},
   "source": [
    "When alpha is too low, it's as good as no penalization and model will be overfitting, complex. Alternatively, when alpha is too high, model will underfit and will become to simple. So alpha needs to be tunded. This can be done using RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6c73570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e251d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ridge regression with 3 alpha values\n",
    "ridge_cv=RidgeCV(alphas=[0.1,1.0,5,7,10,20,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bad5769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "model_ridgeCV=ridge_cv.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a185cdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89015213,  1.01207982,  0.03670572,  0.69670303, -1.92608614,\n",
       "        2.71265969, -0.00928493, -2.97506515,  2.3446415 , -1.78380629,\n",
       "       -2.02178301,  0.84709801, -3.68122188])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridgeCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cdd45289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View alpha\n",
    "model_ridgeCV.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "41b29852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23.718112644972546"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridgeCV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e828894",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c52343a",
   "metadata": {},
   "source": [
    "Lasso is also used for feature selection. In lasso regression, the shrinkage penalty is a hyperparameter multiplied by sum of absolute value of all the coefficients: RSS + alpha * summation of |B|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03a50d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13c81eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lasso with alpha value\n",
    "regression_l=Lasso(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "055f6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso=regression_l.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4970cb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11526463,  0.        , -0.        ,  0.39707879, -0.        ,\n",
       "        2.97425861, -0.        , -0.17056942, -0.        , -0.        ,\n",
       "       -1.59844856,  0.54313871, -3.66614361])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15ff4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e3d9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lasso Regression with apha values\n",
    "lasso_cv=LassoCV(alphas=[0.05,0.5,1,5,7,10,20,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21da81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the linear regression\n",
    "model_lassoCV=lasso_cv.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7872e828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.78186122,  0.88839401, -0.        ,  0.67374712, -1.79294898,\n",
       "        2.74738475, -0.        , -2.78126471,  1.90060723, -1.41445404,\n",
       "       -1.98475002,  0.80473834, -3.72701745])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View co-efficeints\n",
    "model_lassoCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0121989b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View alpha\n",
    "model_lassoCV.alpha_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
